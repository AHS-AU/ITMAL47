{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise L07 - Gridsearch\n",
    "\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "There are two code cells below: 1:) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism (without going into to much detail)\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "#from libitmal import utils as itmalutils\n",
    "from libitmal import dataloaders_v2 as itmaldataloaders\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            r=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                t = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(r)>0:\n",
    "                    r += ','\n",
    "                r += f'{key}={t}{value}{t}'  \n",
    "            return r            \n",
    "        try:\n",
    "            p = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + p + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "            if i == 20:\n",
    "                break;\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(fetchmode=False)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}'\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 2.65 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285714\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.971 (+/-0.048) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.952 (+/-0.084) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.952 (+/-0.084) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.971 (+/-0.048) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris') # or 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=\"scale\")\n",
    "\n",
    "# Setup tuning parameters\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "# cross validation\n",
    "CV=5\n",
    "\n",
    "# Verbose logging\n",
    "VERBOSE=0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "\n",
    "# The gridSearchCV function, is a way of \n",
    "\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code summary:\n",
    "The above code seeks to find the best fitted hyper parameters for the given model. In this example the support-vector classifier (SVC) is the one in play. The SVC model has a dozen of different parameteres (Hyper Parameters) which each can be tweaked and twisted in order for the model to better fit the data. To find the best suited combination of hyper parameters one would have to check each parameter against one another and calculate a score of how well the model is fit for the data.\n",
    "\n",
    "Fortuanaly this process can be automatized using various methods. In this examples we use the GridsearchCV function. A grid search is basically brute forcing every possible combination of hyper parameters and finds the one with the highest score. To limit the amount of checks, one can pick which hyper parameters that have to be checked, in this case the parameters \"kernel\" and \"C\".\n",
    "\n",
    "After the process is done the calculated best combinations on the iris data is with the hyperparameters of \"kernel = linear\" and \"C = 1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SDG` in the search-space before it begins to take considerable compute time doing the full grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 24.03 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 1e-05, 'loss': 'squared_hinge', 'penalty': 'l1'}\n",
      "\tbest 'f1_micro' score=0.9904761904761905\n",
      "\tbest index=81\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.1, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=1e-05, learning_rate='optimal', loss='squared_hinge',\n",
      "       max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.800 (+/-0.133) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[ 1]: 0.752 (+/-0.093) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[ 2]: 0.686 (+/-0.091) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[ 3]: 0.790 (+/-0.203) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[ 4]: 0.733 (+/-0.137) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[ 5]: 0.762 (+/-0.183) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[ 6]: 0.943 (+/-0.072) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[ 7]: 0.714 (+/-0.122) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[ 8]: 0.943 (+/-0.141) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[ 9]: 0.867 (+/-0.232) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_hinge', 'penalty': 'l1'}\n",
      "\t[10]: 0.667 (+/-0.370) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "\t[11]: 0.886 (+/-0.211) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_hinge', 'penalty': 'elasticnet'}\n",
      "\t[12]: 0.886 (+/-0.099) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'perceptron', 'penalty': 'l1'}\n",
      "\t[13]: 0.686 (+/-0.374) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'perceptron', 'penalty': 'l2'}\n",
      "\t[14]: 0.743 (+/-0.167) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'perceptron', 'penalty': 'elasticnet'}\n",
      "\t[15]: 0.343 (+/-0.079) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_loss', 'penalty': 'l1'}\n",
      "\t[16]: 0.381 (+/-0.104) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_loss', 'penalty': 'l2'}\n",
      "\t[17]: 0.295 (+/-0.279) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.5, 'loss': 'squared_loss', 'penalty': 'elasticnet'}\n",
      "\t[18]: 0.781 (+/-0.170) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.1, 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[19]: 0.752 (+/-0.134) for {'alpha': 0.1, 'epsilon': 0.1, 'l1_ratio': 0.1, 'loss': 'log', 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.85      1.00      0.92        11\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.1, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=1e-05, learning_rate='optimal', loss='squared_hinge',\n",
      "       max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "best: dat=iris, score=0.99048, model=SGDClassifier(alpha=0.1,epsilon=0.1,l1_ratio=1e-05,loss='squared_hinge',penalty='l1')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qb..\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Setup search parameters\n",
    "# model = svm.SVC(gamma=\"scale\")\n",
    "model = SGDClassifier() #alpha=0.01,verbose=0.1,eta0=0.1,power_t=0.0\n",
    "# model = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'epsilon': [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001],  \n",
    "    'loss': ('log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss'),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001],\n",
    "    'l1_ratio': [0.5, 0.1, 0.01, 0.001, 0.00001, 0.000001]\n",
    "}\n",
    "\n",
    "CV=5 \n",
    "VERBOSE=0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "\n",
    "# The gridSearchCV function, is a way of \n",
    "\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "As seen in the code, four hyper parameters for the model is chosen (epsilon, loss, penalty, alpha). \n",
    "After doing a grid search, which resulted in over 4000 different combinations, the best model with a score of 0.99048 was found.\n",
    "\n",
    "Best combination of chosen hyper parameters for the SGDClassifier model using iris data: (alpha=0.1,epsilon=0.1,l1_ratio=1e05,loss='squared_hinge',penalty='l1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"Figs/randomsearch.png\" style=\"width:300px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use the same parameters for the random search, but add and investigate the new `n_iter` parameter\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, random_state=42, n_iter=20, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "```\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in seconds (for the iris tiny-data).\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 1.16 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'penalty': 'l1', 'loss': 'modified_huber', 'l1_ratio': 1e-05, 'epsilon': 0.0001, 'alpha': 0.1}\n",
      "\tbest 'f1_micro' score=0.9619047619047619\n",
      "\tbest index=29\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.1, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.0001, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=1e-05, learning_rate='optimal', loss='modified_huber',\n",
      "       max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.733 (+/-0.225) for {'penalty': 'l1', 'loss': 'perceptron', 'l1_ratio': 1e-05, 'epsilon': 1e-06, 'alpha': 1e-06}\n",
      "\t[ 1]: 0.705 (+/-0.233) for {'penalty': 'elasticnet', 'loss': 'perceptron', 'l1_ratio': 0.01, 'epsilon': 0.0001, 'alpha': 0.0001}\n",
      "\t[ 2]: 0.324 (+/-0.051) for {'penalty': 'elasticnet', 'loss': 'squared_loss', 'l1_ratio': 0.01, 'epsilon': 1e-05, 'alpha': 0.0001}\n",
      "\t[ 3]: 0.695 (+/-0.251) for {'penalty': 'elasticnet', 'loss': 'squared_hinge', 'l1_ratio': 0.01, 'epsilon': 1e-06, 'alpha': 0.0001}\n",
      "\t[ 4]: 0.867 (+/-0.104) for {'penalty': 'l1', 'loss': 'squared_hinge', 'l1_ratio': 1e-06, 'epsilon': 0.001, 'alpha': 0.1}\n",
      "\t[ 5]: 0.676 (+/-0.062) for {'penalty': 'l2', 'loss': 'hinge', 'l1_ratio': 1e-05, 'epsilon': 0.0001, 'alpha': 1e-06}\n",
      "\t[ 6]: 0.933 (+/-0.138) for {'penalty': 'l1', 'loss': 'squared_hinge', 'l1_ratio': 0.1, 'epsilon': 0.0001, 'alpha': 0.1}\n",
      "\t[ 7]: 0.648 (+/-0.059) for {'penalty': 'elasticnet', 'loss': 'hinge', 'l1_ratio': 0.01, 'epsilon': 0.01, 'alpha': 0.1}\n",
      "\t[ 8]: 0.333 (+/-0.069) for {'penalty': 'l2', 'loss': 'squared_loss', 'l1_ratio': 0.5, 'epsilon': 0.0001, 'alpha': 0.0001}\n",
      "\t[ 9]: 0.762 (+/-0.239) for {'penalty': 'l2', 'loss': 'modified_huber', 'l1_ratio': 0.1, 'epsilon': 1e-05, 'alpha': 0.1}\n",
      "\t[10]: 0.667 (+/-0.214) for {'penalty': 'l2', 'loss': 'log', 'l1_ratio': 0.1, 'epsilon': 1e-06, 'alpha': 1e-06}\n",
      "\t[11]: 0.762 (+/-0.239) for {'penalty': 'elasticnet', 'loss': 'hinge', 'l1_ratio': 1e-06, 'epsilon': 0.01, 'alpha': 0.001}\n",
      "\t[12]: 0.657 (+/-0.395) for {'penalty': 'l2', 'loss': 'perceptron', 'l1_ratio': 0.1, 'epsilon': 0.0001, 'alpha': 0.001}\n",
      "\t[13]: 0.886 (+/-0.200) for {'penalty': 'l2', 'loss': 'squared_hinge', 'l1_ratio': 0.1, 'epsilon': 1e-06, 'alpha': 0.1}\n",
      "\t[14]: 0.381 (+/-0.211) for {'penalty': 'elasticnet', 'loss': 'squared_loss', 'l1_ratio': 0.001, 'epsilon': 0.01, 'alpha': 0.1}\n",
      "\t[15]: 0.857 (+/-0.250) for {'penalty': 'elasticnet', 'loss': 'perceptron', 'l1_ratio': 0.5, 'epsilon': 0.1, 'alpha': 0.1}\n",
      "\t[16]: 0.248 (+/-0.273) for {'penalty': 'l1', 'loss': 'squared_loss', 'l1_ratio': 0.1, 'epsilon': 1e-05, 'alpha': 0.01}\n",
      "\t[17]: 0.724 (+/-0.191) for {'penalty': 'l2', 'loss': 'modified_huber', 'l1_ratio': 1e-05, 'epsilon': 0.0001, 'alpha': 0.0001}\n",
      "\t[18]: 0.400 (+/-0.318) for {'penalty': 'elasticnet', 'loss': 'squared_loss', 'l1_ratio': 0.001, 'epsilon': 0.01, 'alpha': 1e-05}\n",
      "\t[19]: 0.686 (+/-0.337) for {'penalty': 'l1', 'loss': 'modified_huber', 'l1_ratio': 0.01, 'epsilon': 0.01, 'alpha': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.85      1.00      0.92        11\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.1, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.0001, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=1e-05, learning_rate='optimal', loss='modified_huber',\n",
      "       max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "best: dat=iris, score=0.96190, model=SGDClassifier(alpha=0.1,epsilon=0.0001,l1_ratio=1e-05,loss='modified_huber',penalty='l1')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qc..\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, random_state=42, n_iter=100, cv=CV,\n",
    "                                  scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "# The gridSearchCV function, is a way of \n",
    "\n",
    "random_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(random_tuned , X_test, y_test, t)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "This time instead of using the method grid search method a random search was used. This process is way lighter, but gives a more questionable result.\n",
    "\n",
    "After running the code a couple of times, it seems that the score, using the random search, usually is about 0.05 below the grid search. However it takes around 0.15 seconds to execute whereas the grid search takes around 3 seconds. This is 20 times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Search Quest\n",
    "\n",
    "Finally, we create a small competition: who can find the best model+hyperparameters for MNIST dataset?\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the _tiny-data_ iris: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for an exhaustive grid search, or a faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet, but keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale you input data for some models to perform better (neural networks in particular).\n",
    "\n",
    "* DO NOT USE Keras or Tensorflow models...not yet, and there are too many examples on the net to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=iris, score=0.97143, model=SVC(C=1, kernel='linear')\n",
    "```\n",
    "Now, check if your score (for MNIST) is better that the currently best score on Blackboard: \"L07: Optimization and searching\" | \"Search Quest for MNIST\"\n",
    "\n",
    "> https://blackboard.au.dk/webapps/blackboard/content/listContentEditable.jsp?content_id=_2117394_1&course_id=_124256_1&content_id=_2179138_1\n",
    "\n",
    "and paste your best model into the message box, like\n",
    "```\n",
    "best(Mr.Itmal): dat=mnist, score=0.47090, model=MLPClassifier(random_state=42, max_iter=10, activation='tanh')\n",
    "```\n",
    "Remember to provide a _custom_ name manually, like 'best(joe)', 'best(john)' or 'best(it256)', so we can identify a winnner!\n",
    "\n",
    "For the journal, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: mnist..\n",
      "  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n",
      "  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n",
      "  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n",
      "\n",
      "SEARCH TIME: 1612.69 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 1e-06, 'activation': 'relu'}\n",
      "\tbest 'f1_micro' score=0.9599387755102041\n",
      "\tbest index=3\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tMLPClassifier(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(50, 50, 50), learning_rate='adaptive',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.942 (+/-0.003) for {'solver': 'sgd', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.05, 'activation': 'tanh'}\n",
      "\t[ 1]: 0.953 (+/-0.007) for {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (20, 50, 100, 100, 50, 20), 'alpha': 1e-06, 'activation': 'relu'}\n",
      "\t[ 2]: 0.930 (+/-0.011) for {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.05, 'activation': 'tanh'}\n",
      "\t[ 3]: 0.960 (+/-0.002) for {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 1e-06, 'activation': 'relu'}\n",
      "\t[ 4]: 0.904 (+/-0.016) for {'solver': 'sgd', 'learning_rate': 'constant', 'hidden_layer_sizes': (20, 50, 100, 100, 50, 20), 'alpha': 0.05, 'activation': 'tanh'}\n",
      "\t[ 5]: 0.928 (+/-0.010) for {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.0001, 'activation': 'tanh'}\n",
      "\t[ 6]: 0.929 (+/-0.010) for {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.05, 'activation': 'tanh'}\n",
      "\t[ 7]: 0.931 (+/-0.014) for {'solver': 'sgd', 'learning_rate': 'constant', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 1e-06, 'activation': 'tanh'}\n",
      "\t[ 8]: 0.906 (+/-0.012) for {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (20, 50, 100, 100, 50, 20), 'alpha': 0.0001, 'activation': 'tanh'}\n",
      "\t[ 9]: 0.960 (+/-0.009) for {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.05, 'activation': 'relu'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      2077\n",
      "           1       0.98      0.99      0.99      2385\n",
      "           2       0.95      0.97      0.96      2115\n",
      "           3       0.95      0.96      0.96      2117\n",
      "           4       0.97      0.95      0.96      2004\n",
      "           5       0.96      0.95      0.96      1900\n",
      "           6       0.97      0.97      0.97      2045\n",
      "           7       0.97      0.96      0.97      2189\n",
      "           8       0.95      0.95      0.95      2042\n",
      "           9       0.93      0.96      0.94      2126\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     21000\n",
      "   macro avg       0.96      0.96      0.96     21000\n",
      "weighted avg       0.96      0.96      0.96     21000\n",
      "\n",
      "\n",
      "CTOR for best model: MLPClassifier(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(50, 50, 50), learning_rate='adaptive',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "best: dat=mnist, score=0.95994, model=MLPClassifier(activation='relu',alpha=1e-06,hidden_layer_sizes=(50, 50, 50),learning_rate='adaptive',solver='adam')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qd..\n",
    "# Setup data\n",
    "\n",
    "#import importlib \n",
    "#importlib.reload(itmaldataloaders)\n",
    "\n",
    "\n",
    "from libitmal import kernelfuns as itmalkernelfuns\n",
    "itmalkernelfuns.EnableGPU()\n",
    "\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n",
    "\n",
    "model = MLPClassifier(max_iter=100)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'hidden_layer_sizes': [(20, 50, 100, 100, 50, 20), (50,50,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.000001, 0.05],\n",
    "    'learning_rate' : ('constant', 'adaptive'),\n",
    "    'solver' : ('sgd', 'adam')\n",
    "}\n",
    "\n",
    "\n",
    "start = time()\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, n_iter=10, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "# The gridSearchCV function, is a way of \n",
    "\n",
    "random_tuned.fit(X_train, y_train)\n",
    "\n",
    "t = time()-start\n",
    "\n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, t)\n",
    "\n",
    "# best: dat=mnist, score=0.95933, model=MLPClassifier(activation='relu',alpha=0.05,hidden_layer_sizes=(50, 50, 50),learning_rate='adaptive',solver='adam')\n",
    "\n",
    "\n",
    "#print(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "This exercise was much like the previous, however instead of re-using the model from the other exercises we chose a new model (MLPClassifier) which supports neural networks. By using neural networks one can achieve a significantly more precise model, however the training becomes equally heavy ( to the point were a single process took over 12 minutes ).\n",
    "\n",
    "Due to the heaviness of the process and the amount of data in mnist, not a lot of combinations were tested. The best found combination was: \n",
    "\n",
    "MLPClassifier(activation='relu',alpha=1e-06,hidden_layer_sizes=(50, 50, 50),learning_rate='adaptive',solver='adam') with a score of 0.959943"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
